{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mshin\\Anaconda3\\envs\\econ142\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt     # for plotting\n",
    "\n",
    "import statsmodels.api as sm    # StatsModels\n",
    "\n",
    "# StatsModels bug-fix\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data file\n",
    "data = 'math_10.out'\n",
    "\n",
    "# Data types\n",
    "col_dtypes = {\n",
    "    'c10_zmath': float,\n",
    "    'wgt10_math': float,\n",
    "    'sat': int,\n",
    "    'constant': int,\n",
    "    'c08_zlang': float,\n",
    "    'c08_zmath': float,\n",
    "    'feeder_school': int,\n",
    "    'm_id_pairs': int\n",
    "}\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv(data,\n",
    "                 dtype=col_dtypes,\n",
    "                 na_values='',\n",
    "                 engine='c',\n",
    "                 sep='\\t',\n",
    "                 encoding='utf-8')\n",
    "\n",
    "# Construct a list of all matched SAT-CEB village pairs in the dataset\n",
    "included_pairs = sorted(df['m_id_pairs'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form dummies for included matched SAT/CEB pairs\n",
    "pair_dums = pd.get_dummies(df['m_id_pairs'].astype('category'), \n",
    "                           prefix='mp')\n",
    "\n",
    "# Concatenate matched pair dummies onto dataframe\n",
    "df = pd.concat([df, pair_dums], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c08_zmath</td>    <th>  R-squared:         </th> <td>   0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   218.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>1.39e-66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:55</td>     <th>  Log-Likelihood:    </th> <td> -965.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2013.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2200.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.2664</td> <td>    0.382</td> <td>   -0.697</td> <td> 0.488</td> <td>   -1.027</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.0416</td> <td>    0.116</td> <td>    0.358</td> <td> 0.721</td> <td>   -0.190</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.1473</td> <td>    0.456</td> <td>    0.323</td> <td> 0.747</td> <td>   -0.760</td> <td>    1.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    1.2714</td> <td>    0.384</td> <td>    3.307</td> <td> 0.001</td> <td>    0.506</td> <td>    2.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.3375</td> <td>    0.439</td> <td>    0.769</td> <td> 0.444</td> <td>   -0.536</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.0137</td> <td>    0.384</td> <td>    0.036</td> <td> 0.972</td> <td>   -0.750</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    0.4265</td> <td>    0.394</td> <td>    1.082</td> <td> 0.283</td> <td>   -0.358</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.7381</td> <td>    0.410</td> <td>    1.801</td> <td> 0.076</td> <td>   -0.078</td> <td>    1.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.6417</td> <td>    0.783</td> <td>    0.820</td> <td> 0.415</td> <td>   -0.917</td> <td>    2.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.8942</td> <td>    0.381</td> <td>    2.345</td> <td> 0.022</td> <td>    0.135</td> <td>    1.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.9155</td> <td>    0.432</td> <td>    2.120</td> <td> 0.037</td> <td>    0.056</td> <td>    1.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>   -0.3223</td> <td>    0.478</td> <td>   -0.674</td> <td> 0.502</td> <td>   -1.275</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.1861</td> <td>    0.377</td> <td>    0.493</td> <td> 0.623</td> <td>   -0.565</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -1.0858</td> <td>    0.378</td> <td>   -2.872</td> <td> 0.005</td> <td>   -1.838</td> <td>   -0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>   -0.2624</td> <td>    0.377</td> <td>   -0.695</td> <td> 0.489</td> <td>   -1.013</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.5475</td> <td>    0.635</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.717</td> <td>    1.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>   -0.2590</td> <td>    0.532</td> <td>   -0.487</td> <td> 0.627</td> <td>   -1.317</td> <td>    0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.5116</td> <td>    0.396</td> <td>    1.293</td> <td> 0.200</td> <td>   -0.276</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>   -0.3545</td> <td>    0.386</td> <td>   -0.920</td> <td> 0.361</td> <td>   -1.122</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.7272</td> <td>    0.382</td> <td>    1.904</td> <td> 0.061</td> <td>   -0.033</td> <td>    1.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>   -0.0439</td> <td>    0.624</td> <td>   -0.070</td> <td> 0.944</td> <td>   -1.286</td> <td>    1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.5760</td> <td>    0.398</td> <td>    1.446</td> <td> 0.152</td> <td>   -0.217</td> <td>    1.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    0.2214</td> <td>    0.398</td> <td>    0.557</td> <td> 0.579</td> <td>   -0.570</td> <td>    1.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.4236</td> <td>    0.376</td> <td>    1.126</td> <td> 0.264</td> <td>   -0.325</td> <td>    1.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.0452</td> <td>    0.376</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.793</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>   -0.1423</td> <td>    0.376</td> <td>   -0.378</td> <td> 0.706</td> <td>   -0.891</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>   -0.2757</td> <td>    0.573</td> <td>   -0.481</td> <td> 0.632</td> <td>   -1.417</td> <td>    0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    0.5423</td> <td>    0.686</td> <td>    0.791</td> <td> 0.432</td> <td>   -0.823</td> <td>    1.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    1.2330</td> <td>    0.521</td> <td>    2.365</td> <td> 0.020</td> <td>    0.195</td> <td>    2.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.4963</td> <td>    0.515</td> <td>    0.964</td> <td> 0.338</td> <td>   -0.528</td> <td>    1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.0825</td> <td>    0.372</td> <td>   -0.221</td> <td> 0.825</td> <td>   -0.823</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.6536</td> <td>    0.369</td> <td>    1.770</td> <td> 0.081</td> <td>   -0.082</td> <td>    1.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.6193</td> <td>    0.925</td> <td>    0.670</td> <td> 0.505</td> <td>   -1.221</td> <td>    2.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.9833</td> <td>    0.377</td> <td>    2.606</td> <td> 0.011</td> <td>    0.232</td> <td>    1.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>    0.4174</td> <td>    0.378</td> <td>    1.104</td> <td> 0.273</td> <td>   -0.335</td> <td>    1.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.0831</td> <td>    0.378</td> <td>    0.220</td> <td> 0.826</td> <td>   -0.669</td> <td>    0.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.3482</td> <td>    0.581</td> <td>    0.600</td> <td> 0.550</td> <td>   -0.807</td> <td>    1.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.1183</td> <td>    0.491</td> <td>    0.241</td> <td> 0.810</td> <td>   -0.859</td> <td>    1.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.3255</td> <td>    0.384</td> <td>    0.848</td> <td> 0.399</td> <td>   -0.439</td> <td>    1.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.4557</td> <td>    0.375</td> <td>    1.216</td> <td> 0.228</td> <td>   -0.290</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.3141</td> <td>    0.499</td> <td>    0.629</td> <td> 0.531</td> <td>   -0.680</td> <td>    1.308</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.631</td> <th>  Durbin-Watson:     </th> <td>   1.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.442</td> <th>  Jarque-Bera (JB):  </th> <td>   1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.043</td> <th>  Prob(JB):          </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.209</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c08_zmath</td>    <th>  R-squared:         </th> <td>   0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   218.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>1.39e-66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:55</td>     <th>  Log-Likelihood:    </th> <td> -965.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2013.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2200.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.2664</td> <td>    0.382</td> <td>   -0.697</td> <td> 0.488</td> <td>   -1.027</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.0416</td> <td>    0.116</td> <td>    0.358</td> <td> 0.721</td> <td>   -0.190</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.1473</td> <td>    0.456</td> <td>    0.323</td> <td> 0.747</td> <td>   -0.760</td> <td>    1.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    1.2714</td> <td>    0.384</td> <td>    3.307</td> <td> 0.001</td> <td>    0.506</td> <td>    2.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.3375</td> <td>    0.439</td> <td>    0.769</td> <td> 0.444</td> <td>   -0.536</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.0137</td> <td>    0.384</td> <td>    0.036</td> <td> 0.972</td> <td>   -0.750</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    0.4265</td> <td>    0.394</td> <td>    1.082</td> <td> 0.283</td> <td>   -0.358</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.7381</td> <td>    0.410</td> <td>    1.801</td> <td> 0.076</td> <td>   -0.078</td> <td>    1.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.6417</td> <td>    0.783</td> <td>    0.820</td> <td> 0.415</td> <td>   -0.917</td> <td>    2.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.8942</td> <td>    0.381</td> <td>    2.345</td> <td> 0.022</td> <td>    0.135</td> <td>    1.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.9155</td> <td>    0.432</td> <td>    2.120</td> <td> 0.037</td> <td>    0.056</td> <td>    1.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>   -0.3223</td> <td>    0.478</td> <td>   -0.674</td> <td> 0.502</td> <td>   -1.275</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.1861</td> <td>    0.377</td> <td>    0.493</td> <td> 0.623</td> <td>   -0.565</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -1.0858</td> <td>    0.378</td> <td>   -2.872</td> <td> 0.005</td> <td>   -1.838</td> <td>   -0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>   -0.2624</td> <td>    0.377</td> <td>   -0.695</td> <td> 0.489</td> <td>   -1.013</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.5475</td> <td>    0.635</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.717</td> <td>    1.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>   -0.2590</td> <td>    0.532</td> <td>   -0.487</td> <td> 0.627</td> <td>   -1.317</td> <td>    0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.5116</td> <td>    0.396</td> <td>    1.293</td> <td> 0.200</td> <td>   -0.276</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>   -0.3545</td> <td>    0.386</td> <td>   -0.920</td> <td> 0.361</td> <td>   -1.122</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.7272</td> <td>    0.382</td> <td>    1.904</td> <td> 0.061</td> <td>   -0.033</td> <td>    1.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>   -0.0439</td> <td>    0.624</td> <td>   -0.070</td> <td> 0.944</td> <td>   -1.286</td> <td>    1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.5760</td> <td>    0.398</td> <td>    1.446</td> <td> 0.152</td> <td>   -0.217</td> <td>    1.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    0.2214</td> <td>    0.398</td> <td>    0.557</td> <td> 0.579</td> <td>   -0.570</td> <td>    1.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.4236</td> <td>    0.376</td> <td>    1.126</td> <td> 0.264</td> <td>   -0.325</td> <td>    1.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.0452</td> <td>    0.376</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.793</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>   -0.1423</td> <td>    0.376</td> <td>   -0.378</td> <td> 0.706</td> <td>   -0.891</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>   -0.2757</td> <td>    0.573</td> <td>   -0.481</td> <td> 0.632</td> <td>   -1.417</td> <td>    0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    0.5423</td> <td>    0.686</td> <td>    0.791</td> <td> 0.432</td> <td>   -0.823</td> <td>    1.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    1.2330</td> <td>    0.521</td> <td>    2.365</td> <td> 0.020</td> <td>    0.195</td> <td>    2.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.4963</td> <td>    0.515</td> <td>    0.964</td> <td> 0.338</td> <td>   -0.528</td> <td>    1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.0825</td> <td>    0.372</td> <td>   -0.221</td> <td> 0.825</td> <td>   -0.823</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.6536</td> <td>    0.369</td> <td>    1.770</td> <td> 0.081</td> <td>   -0.082</td> <td>    1.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.6193</td> <td>    0.925</td> <td>    0.670</td> <td> 0.505</td> <td>   -1.221</td> <td>    2.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.9833</td> <td>    0.377</td> <td>    2.606</td> <td> 0.011</td> <td>    0.232</td> <td>    1.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>    0.4174</td> <td>    0.378</td> <td>    1.104</td> <td> 0.273</td> <td>   -0.335</td> <td>    1.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.0831</td> <td>    0.378</td> <td>    0.220</td> <td> 0.826</td> <td>   -0.669</td> <td>    0.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.3482</td> <td>    0.581</td> <td>    0.600</td> <td> 0.550</td> <td>   -0.807</td> <td>    1.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.1183</td> <td>    0.491</td> <td>    0.241</td> <td> 0.810</td> <td>   -0.859</td> <td>    1.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.3255</td> <td>    0.384</td> <td>    0.848</td> <td> 0.399</td> <td>   -0.439</td> <td>    1.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.4557</td> <td>    0.375</td> <td>    1.216</td> <td> 0.228</td> <td>   -0.290</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.3141</td> <td>    0.499</td> <td>    0.629</td> <td> 0.531</td> <td>   -0.680</td> <td>    1.308</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.631</td> <th>  Durbin-Watson:     </th> <td>   1.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.442</td> <th>  Jarque-Bera (JB):  </th> <td>   1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.043</td> <th>  Prob(JB):          </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.209</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct outcome vector, design matrix and test instrument inverse weights\n",
    "Y = df['c08_zmath']     # outcome\n",
    "test_wgt = 1. / df['wgt10_math']    # test instrument weights\n",
    "X = df[['constant', 'sat']]     # design matrix\n",
    "\n",
    "# omit last matched pair to avoid dummy variable trap\n",
    "X = pd.concat([X, pair_dums.iloc[:, 0:-1]], axis=1)\n",
    "\n",
    "# Compute weighted least squares fit\n",
    "# NOTE: cluster-robust standard errors\n",
    "wls = sm.WLS(Y, X, weights=test_wgt)\\\n",
    "    .fit(cov_type='cluster',\n",
    "         cov_kwds={'groups': df['feeder_school']},\n",
    "         use_t=True)\n",
    "wls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The coefficient on *sat* is 0.0416, and it is not significant even at the 10 percent level. We can conclude that at 2008 baseline, children in SAT villages and in CEB villages had no significant difference in math test scores.\n",
    " - Some of the children dropped out of school and had to be located at their home. Since those who dropped out were not randomly sampled, we can account for this sampling by assigning different weights. In the paper, the authors assigned a weight of 2 to those were located at home, and 1 to the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   48.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>2.02e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:55</td>     <th>  Log-Likelihood:    </th> <td> -1067.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2217.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2404.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.1922</td> <td>    0.206</td> <td>   -0.934</td> <td> 0.353</td> <td>   -0.602</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.1547</td> <td>    0.099</td> <td>    1.560</td> <td> 0.123</td> <td>   -0.043</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.0426</td> <td>    0.224</td> <td>    0.191</td> <td> 0.849</td> <td>   -0.402</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    0.9798</td> <td>    0.226</td> <td>    4.334</td> <td> 0.000</td> <td>    0.530</td> <td>    1.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.8838</td> <td>    0.211</td> <td>    4.197</td> <td> 0.000</td> <td>    0.465</td> <td>    1.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.4992</td> <td>    0.314</td> <td>    1.589</td> <td> 0.116</td> <td>   -0.126</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    1.0689</td> <td>    0.243</td> <td>    4.398</td> <td> 0.000</td> <td>    0.585</td> <td>    1.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.2537</td> <td>    0.334</td> <td>    0.759</td> <td> 0.450</td> <td>   -0.411</td> <td>    0.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.5964</td> <td>    0.297</td> <td>    2.009</td> <td> 0.048</td> <td>    0.006</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.9082</td> <td>    0.195</td> <td>    4.661</td> <td> 0.000</td> <td>    0.520</td> <td>    1.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.7975</td> <td>    0.219</td> <td>    3.637</td> <td> 0.000</td> <td>    0.361</td> <td>    1.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>    0.3924</td> <td>    0.205</td> <td>    1.915</td> <td> 0.059</td> <td>   -0.015</td> <td>    0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.4594</td> <td>    0.203</td> <td>    2.267</td> <td> 0.026</td> <td>    0.056</td> <td>    0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -0.6551</td> <td>    0.239</td> <td>   -2.743</td> <td> 0.008</td> <td>   -1.130</td> <td>   -0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>    0.6984</td> <td>    0.278</td> <td>    2.512</td> <td> 0.014</td> <td>    0.145</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.9563</td> <td>    0.287</td> <td>    3.333</td> <td> 0.001</td> <td>    0.385</td> <td>    1.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>    0.1826</td> <td>    0.672</td> <td>    0.272</td> <td> 0.787</td> <td>   -1.155</td> <td>    1.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.0197</td> <td>    0.246</td> <td>    0.080</td> <td> 0.936</td> <td>   -0.469</td> <td>    0.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>    0.2060</td> <td>    0.317</td> <td>    0.650</td> <td> 0.517</td> <td>   -0.425</td> <td>    0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.6934</td> <td>    0.207</td> <td>    3.344</td> <td> 0.001</td> <td>    0.281</td> <td>    1.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>    0.3406</td> <td>    0.238</td> <td>    1.428</td> <td> 0.157</td> <td>   -0.134</td> <td>    0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.7343</td> <td>    0.646</td> <td>    1.136</td> <td> 0.259</td> <td>   -0.552</td> <td>    2.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    1.1224</td> <td>    0.250</td> <td>    4.481</td> <td> 0.000</td> <td>    0.624</td> <td>    1.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.8081</td> <td>    0.346</td> <td>    2.335</td> <td> 0.022</td> <td>    0.119</td> <td>    1.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.1022</td> <td>    0.231</td> <td>   -0.442</td> <td> 0.660</td> <td>   -0.563</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>    0.4726</td> <td>    0.199</td> <td>    2.373</td> <td> 0.020</td> <td>    0.076</td> <td>    0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>    0.1260</td> <td>    0.438</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.746</td> <td>    0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    1.0243</td> <td>    0.446</td> <td>    2.298</td> <td> 0.024</td> <td>    0.137</td> <td>    1.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    0.7930</td> <td>    0.202</td> <td>    3.928</td> <td> 0.000</td> <td>    0.391</td> <td>    1.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.7258</td> <td>    0.270</td> <td>    2.691</td> <td> 0.009</td> <td>    0.189</td> <td>    1.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.2063</td> <td>    0.198</td> <td>   -1.042</td> <td> 0.300</td> <td>   -0.600</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.8671</td> <td>    0.228</td> <td>    3.804</td> <td> 0.000</td> <td>    0.413</td> <td>    1.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.9330</td> <td>    0.873</td> <td>    1.069</td> <td> 0.288</td> <td>   -0.804</td> <td>    2.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.3070</td> <td>    0.422</td> <td>    0.727</td> <td> 0.469</td> <td>   -0.534</td> <td>    1.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>   -0.0097</td> <td>    0.335</td> <td>   -0.029</td> <td> 0.977</td> <td>   -0.677</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.6283</td> <td>    0.331</td> <td>    1.896</td> <td> 0.062</td> <td>   -0.031</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.5546</td> <td>    0.204</td> <td>    2.721</td> <td> 0.008</td> <td>    0.149</td> <td>    0.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.5983</td> <td>    0.586</td> <td>    1.022</td> <td> 0.310</td> <td>   -0.567</td> <td>    1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.5117</td> <td>    0.347</td> <td>    1.475</td> <td> 0.144</td> <td>   -0.179</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.8047</td> <td>    0.225</td> <td>    3.583</td> <td> 0.001</td> <td>    0.358</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.5474</td> <td>    0.596</td> <td>    0.919</td> <td> 0.361</td> <td>   -0.639</td> <td>    1.733</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.516</td> <th>  Durbin-Watson:     </th> <td>   1.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.172</td> <th>  Jarque-Bera (JB):  </th> <td>   3.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.164</td> <th>  Prob(JB):          </th> <td>   0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.075</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   48.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>2.02e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:55</td>     <th>  Log-Likelihood:    </th> <td> -1067.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2217.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2404.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.1922</td> <td>    0.206</td> <td>   -0.934</td> <td> 0.353</td> <td>   -0.602</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.1547</td> <td>    0.099</td> <td>    1.560</td> <td> 0.123</td> <td>   -0.043</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.0426</td> <td>    0.224</td> <td>    0.191</td> <td> 0.849</td> <td>   -0.402</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    0.9798</td> <td>    0.226</td> <td>    4.334</td> <td> 0.000</td> <td>    0.530</td> <td>    1.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.8838</td> <td>    0.211</td> <td>    4.197</td> <td> 0.000</td> <td>    0.465</td> <td>    1.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.4992</td> <td>    0.314</td> <td>    1.589</td> <td> 0.116</td> <td>   -0.126</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    1.0689</td> <td>    0.243</td> <td>    4.398</td> <td> 0.000</td> <td>    0.585</td> <td>    1.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.2537</td> <td>    0.334</td> <td>    0.759</td> <td> 0.450</td> <td>   -0.411</td> <td>    0.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.5964</td> <td>    0.297</td> <td>    2.009</td> <td> 0.048</td> <td>    0.006</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.9082</td> <td>    0.195</td> <td>    4.661</td> <td> 0.000</td> <td>    0.520</td> <td>    1.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.7975</td> <td>    0.219</td> <td>    3.637</td> <td> 0.000</td> <td>    0.361</td> <td>    1.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>    0.3924</td> <td>    0.205</td> <td>    1.915</td> <td> 0.059</td> <td>   -0.015</td> <td>    0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.4594</td> <td>    0.203</td> <td>    2.267</td> <td> 0.026</td> <td>    0.056</td> <td>    0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -0.6551</td> <td>    0.239</td> <td>   -2.743</td> <td> 0.008</td> <td>   -1.130</td> <td>   -0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>    0.6984</td> <td>    0.278</td> <td>    2.512</td> <td> 0.014</td> <td>    0.145</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.9563</td> <td>    0.287</td> <td>    3.333</td> <td> 0.001</td> <td>    0.385</td> <td>    1.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>    0.1826</td> <td>    0.672</td> <td>    0.272</td> <td> 0.787</td> <td>   -1.155</td> <td>    1.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.0197</td> <td>    0.246</td> <td>    0.080</td> <td> 0.936</td> <td>   -0.469</td> <td>    0.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>    0.2060</td> <td>    0.317</td> <td>    0.650</td> <td> 0.517</td> <td>   -0.425</td> <td>    0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.6934</td> <td>    0.207</td> <td>    3.344</td> <td> 0.001</td> <td>    0.281</td> <td>    1.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>    0.3406</td> <td>    0.238</td> <td>    1.428</td> <td> 0.157</td> <td>   -0.134</td> <td>    0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.7343</td> <td>    0.646</td> <td>    1.136</td> <td> 0.259</td> <td>   -0.552</td> <td>    2.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    1.1224</td> <td>    0.250</td> <td>    4.481</td> <td> 0.000</td> <td>    0.624</td> <td>    1.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.8081</td> <td>    0.346</td> <td>    2.335</td> <td> 0.022</td> <td>    0.119</td> <td>    1.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.1022</td> <td>    0.231</td> <td>   -0.442</td> <td> 0.660</td> <td>   -0.563</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>    0.4726</td> <td>    0.199</td> <td>    2.373</td> <td> 0.020</td> <td>    0.076</td> <td>    0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>    0.1260</td> <td>    0.438</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.746</td> <td>    0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    1.0243</td> <td>    0.446</td> <td>    2.298</td> <td> 0.024</td> <td>    0.137</td> <td>    1.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    0.7930</td> <td>    0.202</td> <td>    3.928</td> <td> 0.000</td> <td>    0.391</td> <td>    1.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.7258</td> <td>    0.270</td> <td>    2.691</td> <td> 0.009</td> <td>    0.189</td> <td>    1.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.2063</td> <td>    0.198</td> <td>   -1.042</td> <td> 0.300</td> <td>   -0.600</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.8671</td> <td>    0.228</td> <td>    3.804</td> <td> 0.000</td> <td>    0.413</td> <td>    1.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.9330</td> <td>    0.873</td> <td>    1.069</td> <td> 0.288</td> <td>   -0.804</td> <td>    2.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.3070</td> <td>    0.422</td> <td>    0.727</td> <td> 0.469</td> <td>   -0.534</td> <td>    1.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>   -0.0097</td> <td>    0.335</td> <td>   -0.029</td> <td> 0.977</td> <td>   -0.677</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.6283</td> <td>    0.331</td> <td>    1.896</td> <td> 0.062</td> <td>   -0.031</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.5546</td> <td>    0.204</td> <td>    2.721</td> <td> 0.008</td> <td>    0.149</td> <td>    0.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.5983</td> <td>    0.586</td> <td>    1.022</td> <td> 0.310</td> <td>   -0.567</td> <td>    1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.5117</td> <td>    0.347</td> <td>    1.475</td> <td> 0.144</td> <td>   -0.179</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.8047</td> <td>    0.225</td> <td>    3.583</td> <td> 0.001</td> <td>    0.358</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.5474</td> <td>    0.596</td> <td>    0.919</td> <td> 0.361</td> <td>   -0.639</td> <td>    1.733</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.516</td> <th>  Durbin-Watson:     </th> <td>   1.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.172</td> <th>  Jarque-Bera (JB):  </th> <td>   3.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.164</td> <th>  Prob(JB):          </th> <td>   0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.075</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now independent variable is c10_zmath\n",
    "Y = df['c10_zmath']\n",
    "\n",
    "# WLS\n",
    "wls = sm.WLS(Y, X, weights=test_wgt).\\\n",
    "    fit(cov_type='cluster',\n",
    "        cov_kwds={'groups': df['feeder_school']},\n",
    "        use_t=True)\n",
    "wls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The coefficient on *sat* is 0.1547, but it is not significant even at the 10 percent level. We cannot conclude that residing in a SAT village resulted in higher 2010 math test scores than residing in a CEB village. This is the first specifictation that was used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   69.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>8.57e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:56</td>     <th>  Log-Likelihood:    </th> <td> -902.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   1891.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   670</td>      <th>  BIC:               </th> <td>   2087.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    42</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    0.0319</td> <td>    0.055</td> <td>    0.579</td> <td> 0.564</td> <td>   -0.078</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>       <td>    0.1342</td> <td>    0.082</td> <td>    1.636</td> <td> 0.106</td> <td>   -0.029</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>      <td>   -0.1218</td> <td>    0.246</td> <td>   -0.495</td> <td> 0.622</td> <td>   -0.612</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>      <td>   -0.0918</td> <td>    0.087</td> <td>   -1.059</td> <td> 0.293</td> <td>   -0.264</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>      <td>    0.6063</td> <td>    0.276</td> <td>    2.198</td> <td> 0.031</td> <td>    0.057</td> <td>    1.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>      <td>    0.4544</td> <td>    0.165</td> <td>    2.758</td> <td> 0.007</td> <td>    0.126</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>      <td>    0.7510</td> <td>    0.048</td> <td>   15.508</td> <td> 0.000</td> <td>    0.655</td> <td>    0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>     <td>   -0.3037</td> <td>    0.112</td> <td>   -2.703</td> <td> 0.008</td> <td>   -0.527</td> <td>   -0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>     <td>    0.2819</td> <td>    0.239</td> <td>    1.180</td> <td> 0.242</td> <td>   -0.194</td> <td>    0.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>     <td>    0.2218</td> <td>    0.097</td> <td>    2.296</td> <td> 0.024</td> <td>    0.030</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>     <td>    0.2058</td> <td>    0.049</td> <td>    4.194</td> <td> 0.000</td> <td>    0.108</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>     <td>    0.5164</td> <td>    0.263</td> <td>    1.965</td> <td> 0.053</td> <td>   -0.007</td> <td>    1.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>     <td>    0.3471</td> <td>    0.071</td> <td>    4.858</td> <td> 0.000</td> <td>    0.205</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>     <td>    0.1698</td> <td>    0.133</td> <td>    1.272</td> <td> 0.207</td> <td>   -0.096</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>     <td>    0.7460</td> <td>    0.209</td> <td>    3.566</td> <td> 0.001</td> <td>    0.330</td> <td>    1.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>     <td>    0.4745</td> <td>    0.167</td> <td>    2.849</td> <td> 0.006</td> <td>    0.143</td> <td>    0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>     <td>    0.2258</td> <td>    0.404</td> <td>    0.560</td> <td> 0.577</td> <td>   -0.577</td> <td>    1.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>     <td>   -0.4286</td> <td>    0.162</td> <td>   -2.645</td> <td> 0.010</td> <td>   -0.751</td> <td>   -0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>     <td>    0.3738</td> <td>    0.161</td> <td>    2.321</td> <td> 0.023</td> <td>    0.053</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>     <td>    0.2733</td> <td>    0.140</td> <td>    1.953</td> <td> 0.054</td> <td>   -0.005</td> <td>    0.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>     <td>    0.4040</td> <td>    0.154</td> <td>    2.629</td> <td> 0.010</td> <td>    0.098</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>     <td>    0.2758</td> <td>    0.473</td> <td>    0.583</td> <td> 0.562</td> <td>   -0.666</td> <td>    1.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>     <td>    0.9408</td> <td>    0.102</td> <td>    9.259</td> <td> 0.000</td> <td>    0.739</td> <td>    1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>     <td>    0.4265</td> <td>    0.252</td> <td>    1.693</td> <td> 0.094</td> <td>   -0.075</td> <td>    0.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>     <td>    0.0006</td> <td>    0.083</td> <td>    0.007</td> <td> 0.995</td> <td>   -0.165</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>     <td>    0.5594</td> <td>    0.040</td> <td>   13.922</td> <td> 0.000</td> <td>    0.479</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>     <td>    0.1787</td> <td>    0.121</td> <td>    1.480</td> <td> 0.143</td> <td>   -0.062</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>     <td>    0.5310</td> <td>    0.051</td> <td>   10.413</td> <td> 0.000</td> <td>    0.429</td> <td>    0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>     <td>   -0.1882</td> <td>    0.183</td> <td>   -1.028</td> <td> 0.307</td> <td>   -0.553</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>     <td>    0.3051</td> <td>    0.062</td> <td>    4.940</td> <td> 0.000</td> <td>    0.182</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>     <td>   -0.1855</td> <td>    0.100</td> <td>   -1.853</td> <td> 0.068</td> <td>   -0.385</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>     <td>    0.4251</td> <td>    0.141</td> <td>    3.005</td> <td> 0.004</td> <td>    0.144</td> <td>    0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>     <td>    0.4729</td> <td>    0.251</td> <td>    1.884</td> <td> 0.063</td> <td>   -0.027</td> <td>    0.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>     <td>   -0.3124</td> <td>    0.253</td> <td>   -1.233</td> <td> 0.221</td> <td>   -0.817</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>     <td>   -0.3160</td> <td>    0.257</td> <td>   -1.230</td> <td> 0.223</td> <td>   -0.828</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>     <td>    0.5660</td> <td>    0.235</td> <td>    2.408</td> <td> 0.018</td> <td>    0.098</td> <td>    1.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>     <td>    0.3310</td> <td>    0.402</td> <td>    0.823</td> <td> 0.413</td> <td>   -0.470</td> <td>    1.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>     <td>    0.4189</td> <td>    0.304</td> <td>    1.376</td> <td> 0.173</td> <td>   -0.187</td> <td>    1.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>     <td>    0.3497</td> <td>    0.206</td> <td>    1.701</td> <td> 0.093</td> <td>   -0.060</td> <td>    0.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>     <td>    0.6104</td> <td>    0.086</td> <td>    7.092</td> <td> 0.000</td> <td>    0.439</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>     <td>    0.2539</td> <td>    0.379</td> <td>    0.670</td> <td> 0.505</td> <td>   -0.501</td> <td>    1.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zmath</th> <td>    0.5124</td> <td>    0.056</td> <td>    9.173</td> <td> 0.000</td> <td>    0.401</td> <td>    0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zlang</th> <td>    0.2577</td> <td>    0.049</td> <td>    5.264</td> <td> 0.000</td> <td>    0.160</td> <td>    0.355</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.179</td> <th>  Durbin-Watson:     </th> <td>   1.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  23.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.115</td> <th>  Prob(JB):          </th> <td>7.58e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.861</td> <th>  Cond. No.          </th> <td>    39.0</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   69.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>8.57e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:56</td>     <th>  Log-Likelihood:    </th> <td> -902.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   1891.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   670</td>      <th>  BIC:               </th> <td>   2087.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    42</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    0.0319</td> <td>    0.055</td> <td>    0.579</td> <td> 0.564</td> <td>   -0.078</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>       <td>    0.1342</td> <td>    0.082</td> <td>    1.636</td> <td> 0.106</td> <td>   -0.029</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>      <td>   -0.1218</td> <td>    0.246</td> <td>   -0.495</td> <td> 0.622</td> <td>   -0.612</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>      <td>   -0.0918</td> <td>    0.087</td> <td>   -1.059</td> <td> 0.293</td> <td>   -0.264</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>      <td>    0.6063</td> <td>    0.276</td> <td>    2.198</td> <td> 0.031</td> <td>    0.057</td> <td>    1.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>      <td>    0.4544</td> <td>    0.165</td> <td>    2.758</td> <td> 0.007</td> <td>    0.126</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>      <td>    0.7510</td> <td>    0.048</td> <td>   15.508</td> <td> 0.000</td> <td>    0.655</td> <td>    0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>     <td>   -0.3037</td> <td>    0.112</td> <td>   -2.703</td> <td> 0.008</td> <td>   -0.527</td> <td>   -0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>     <td>    0.2819</td> <td>    0.239</td> <td>    1.180</td> <td> 0.242</td> <td>   -0.194</td> <td>    0.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>     <td>    0.2218</td> <td>    0.097</td> <td>    2.296</td> <td> 0.024</td> <td>    0.030</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>     <td>    0.2058</td> <td>    0.049</td> <td>    4.194</td> <td> 0.000</td> <td>    0.108</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>     <td>    0.5164</td> <td>    0.263</td> <td>    1.965</td> <td> 0.053</td> <td>   -0.007</td> <td>    1.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>     <td>    0.3471</td> <td>    0.071</td> <td>    4.858</td> <td> 0.000</td> <td>    0.205</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>     <td>    0.1698</td> <td>    0.133</td> <td>    1.272</td> <td> 0.207</td> <td>   -0.096</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>     <td>    0.7460</td> <td>    0.209</td> <td>    3.566</td> <td> 0.001</td> <td>    0.330</td> <td>    1.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>     <td>    0.4745</td> <td>    0.167</td> <td>    2.849</td> <td> 0.006</td> <td>    0.143</td> <td>    0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>     <td>    0.2258</td> <td>    0.404</td> <td>    0.560</td> <td> 0.577</td> <td>   -0.577</td> <td>    1.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>     <td>   -0.4286</td> <td>    0.162</td> <td>   -2.645</td> <td> 0.010</td> <td>   -0.751</td> <td>   -0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>     <td>    0.3738</td> <td>    0.161</td> <td>    2.321</td> <td> 0.023</td> <td>    0.053</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>     <td>    0.2733</td> <td>    0.140</td> <td>    1.953</td> <td> 0.054</td> <td>   -0.005</td> <td>    0.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>     <td>    0.4040</td> <td>    0.154</td> <td>    2.629</td> <td> 0.010</td> <td>    0.098</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>     <td>    0.2758</td> <td>    0.473</td> <td>    0.583</td> <td> 0.562</td> <td>   -0.666</td> <td>    1.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>     <td>    0.9408</td> <td>    0.102</td> <td>    9.259</td> <td> 0.000</td> <td>    0.739</td> <td>    1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>     <td>    0.4265</td> <td>    0.252</td> <td>    1.693</td> <td> 0.094</td> <td>   -0.075</td> <td>    0.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>     <td>    0.0006</td> <td>    0.083</td> <td>    0.007</td> <td> 0.995</td> <td>   -0.165</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>     <td>    0.5594</td> <td>    0.040</td> <td>   13.922</td> <td> 0.000</td> <td>    0.479</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>     <td>    0.1787</td> <td>    0.121</td> <td>    1.480</td> <td> 0.143</td> <td>   -0.062</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>     <td>    0.5310</td> <td>    0.051</td> <td>   10.413</td> <td> 0.000</td> <td>    0.429</td> <td>    0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>     <td>   -0.1882</td> <td>    0.183</td> <td>   -1.028</td> <td> 0.307</td> <td>   -0.553</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>     <td>    0.3051</td> <td>    0.062</td> <td>    4.940</td> <td> 0.000</td> <td>    0.182</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>     <td>   -0.1855</td> <td>    0.100</td> <td>   -1.853</td> <td> 0.068</td> <td>   -0.385</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>     <td>    0.4251</td> <td>    0.141</td> <td>    3.005</td> <td> 0.004</td> <td>    0.144</td> <td>    0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>     <td>    0.4729</td> <td>    0.251</td> <td>    1.884</td> <td> 0.063</td> <td>   -0.027</td> <td>    0.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>     <td>   -0.3124</td> <td>    0.253</td> <td>   -1.233</td> <td> 0.221</td> <td>   -0.817</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>     <td>   -0.3160</td> <td>    0.257</td> <td>   -1.230</td> <td> 0.223</td> <td>   -0.828</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>     <td>    0.5660</td> <td>    0.235</td> <td>    2.408</td> <td> 0.018</td> <td>    0.098</td> <td>    1.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>     <td>    0.3310</td> <td>    0.402</td> <td>    0.823</td> <td> 0.413</td> <td>   -0.470</td> <td>    1.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>     <td>    0.4189</td> <td>    0.304</td> <td>    1.376</td> <td> 0.173</td> <td>   -0.187</td> <td>    1.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>     <td>    0.3497</td> <td>    0.206</td> <td>    1.701</td> <td> 0.093</td> <td>   -0.060</td> <td>    0.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>     <td>    0.6104</td> <td>    0.086</td> <td>    7.092</td> <td> 0.000</td> <td>    0.439</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>     <td>    0.2539</td> <td>    0.379</td> <td>    0.670</td> <td> 0.505</td> <td>   -0.501</td> <td>    1.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zmath</th> <td>    0.5124</td> <td>    0.056</td> <td>    9.173</td> <td> 0.000</td> <td>    0.401</td> <td>    0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zlang</th> <td>    0.2577</td> <td>    0.049</td> <td>    5.264</td> <td> 0.000</td> <td>    0.160</td> <td>    0.355</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.179</td> <th>  Durbin-Watson:     </th> <td>   1.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  23.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.115</td> <th>  Prob(JB):          </th> <td>7.58e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.861</td> <th>  Cond. No.          </th> <td>    39.0</td>\n",
       "</tr>\n",
       "</table>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional control for c08_zmath, c08_zlang\n",
    "X = pd.concat([X, df[['c08_zmath', 'c08_zlang']]], axis=1)\n",
    "\n",
    "# WLS\n",
    "wls = sm.WLS(Y, X, weights=test_wgt)\\\n",
    "    .fit(cov_type='cluster',\n",
    "         cov_kwds={'groups': df['feeder_school']},\n",
    "         use_t=True)\n",
    "wls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The coefficient on *sat* is 0.1342, but it is not significant even at the 10 percent level. When including student controls, *c08_zmath* and *c08_zlang*, we cannot conlcude that residing in a SAT village resulted in higher 2010 math test scores than residing in a CEB village. In the paper, this is the first specifictation with student controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.627957\n         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1eee750f7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEtJJREFUeJzt3X2QXXV9x/H3F5AmWEiALJYmxAUnohR1wGhpaasG7SgqD4oW1BocNK2lqGCnrA9TUjtOQ2uNdHSsUayB+sCDVtCgDo86ZgoaBI2ACGIaklCIlgSVKEK//eOeNUv4JXt2c885N5v3a2Znzzl7dn+fubu5n9zz8LuRmUiStK09ug4gSRpMFoQkqciCkCQVWRCSpCILQpJUZEFIkoosCElSkQUhSSqyICRJRXt1HWBnzJo1K4eHh7uOIUm7lJtvvvknmTk03n67dEEMDw+zatWqrmNI0i4lIv67zn4eYpIkFVkQkqQiC0KSVGRBSJKKLAhJUpEFIUkqsiAkSUUWhCSpyIKQJBU1did1RHwSeAXwQGYeWW07ALgEGAbWAK/NzAcjIoALgOOBh4HTM/M7TWXbXR275DrWb9rS+rizZ05n5ciC1seVtHOanGrjU8CHgYvGbBsBrs3MJRExUq2fC7wMmFd9/D7w0eqz+mj9pi2sWfLy1scdHlnR+piSdl5jh5gy8xvA/26z+URgebW8HDhpzPaLsudGYGZEHNxUNknS+No+B/GUzLwPoPp8ULV9NnDvmP3WVdskSR0ZlJPUUdiWxR0jFkXEqohYtXHjxoZjSdLuq+2CuH/00FH1+YFq+zrgkDH7zQE2lH5AZi7LzPmZOX9oaNzpzCVJk9R2QVwJLKyWFwJXjNn+xug5Btg8eihKktSNJi9z/SzwQmBWRKwDzgOWAJdGxBnAWuA11e5X0bvE9W56l7m+qalckqR6GiuIzDxtO186rrBvAmc2lUWSNHGDcpJakjRgLAhJUpEFIUkqsiAkSUUWhCSpqMnJ+iSgN5trVxP2OZOsNHkWhBrX5RO0M8lKk+chJklSkQUhSSqyICRJRRaEJKnIgpAkFVkQkqQiC0KSVGRBSJKKLAhJUpEFIUkqsiAkSUUWhCSpyIKQJBVZEJKkIgtCklRkQUiSiiwISVKRBSFJKrIgJElFFoQkqciCkCQVWRCSpCILQpJUZEFIkoosCElSUScFERFnR8RtEfH9iPhsREyLiEMj4qaIuCsiLomIvbvIJknqab0gImI28DZgfmYeCewJnAqcDyzNzHnAg8AZbWeTJG3V1SGmvYDpEbEXsA9wH7AAuLz6+nLgpI6ySZLooCAycz3wAWAtvWLYDNwMbMrMR6vd1gGzS98fEYsiYlVErNq4cWMbkSVpt9TFIab9gROBQ4HfBZ4MvKywa5a+PzOXZeb8zJw/NDTUXFBJ2s11cYjpxcCPM3NjZv4a+ALwh8DM6pATwBxgQwfZJEmVLgpiLXBMROwTEQEcB9wOXA+cUu2zELiig2ySpEoX5yBuoncy+jvA6irDMuBc4JyIuBs4ELiw7WySpK32Gn+X/svM84Dzttl8D/D8DuJIkgq8k1qSVGRBSJKKLAhJUpEFIUkqsiAkSUUWhCSpyIKQJBVZEJKkIgtCklRkQUiSiiwISVKRBSFJKrIgJElFFoQkqciCkCQVWRCSpCILQpJUZEFIkoosCElSkQUhSSqyICRJRRaEJKnIgpAkFe3VdQBJU8jSZ8Hmte2PO2MunL26/XGnuFoFERFHZub3mw4jaRe3eS0s3tz+uItntD/mbqDuIaZ/i4hvRcRfRcTMRhNJkgZCrYLIzD8CXg8cAqyKiM9ExEsaTSZJ6lTtk9SZeRfwXuBc4AXAv0bEDyLiVU2FkyR1p1ZBRMSzI2IpcAewAHhlZj6zWl7aYD5JUkfqXsX0YeDjwLszc8voxszcEBHvbSSZJKlTdQvieGBLZj4GEBF7ANMy8+HMvLixdJKkztQ9B3ENMH3M+j7VtkmJiJkRcXl1DuOOiPiDiDggIq6OiLuqz/tP9udLknZe3YKYlpk/H12plvfZiXEvAL6amc8AnkPv3MYIcG1mzgOurdYlSR2pWxC/iIijR1ci4rnAlh3sv10RsR/wJ8CFAJn5SGZuAk4Elle7LQdOmszPlyT1R91zEO8ALouIDdX6wcCfTXLMw4CNwL9HxHOAm4G3A0/JzPsAMvO+iDhokj9fktQHtQoiM78dEc8ADgcC+EFm/nonxjwaOCszb4qIC5jA4aSIWAQsApg7d+4kI0iSxjOR2VyfBzwbOAo4LSLeOMkx1wHrMvOmav1yeoVxf0QcDFB9fqD0zZm5LDPnZ+b8oaGhSUaQJI2n7mR9FwNPA24FHqs2J3DRRAfMzP+JiHsj4vDMvBM4Dri9+lgILKk+XzHRn62auppxswNrpgFLnelTmoy65yDmA0dkZvZp3LOAT0fE3sA9wJvovZq5NCLOANYCr+nTWNpWVzNudmB4ZAVreF3XMaRdUt2C+D7wO8B9/Rg0M2+lVzrbOq4fP1+StPPqFsQs4PaI+Bbwq9GNmXlCI6kkSZ2rWxCLmwwhSRo8dS9z/XpEPBWYl5nXRMQ+wJ7NRpMkdanudN9voXc56seqTbOBLzYVSpLUvbr3QZwJHAs8BL958yDvdJakKaxuQfwqMx8ZXYmIvejdByFJmqLqFsTXI+LdwPTqvagvA77UXCxJUtfqFsQIvQn2VgN/AVxF7/2pJUlTVN2rmP6P3luOfrzZOJKkQVF3LqYfUzjnkJmH9T2RJGkgTGQuplHT6M2TdED/40iSBkWtcxCZ+dMxH+sz80PAgoazSZI6VPcQ09FjVveg94pi30YSSZIGQt1DTP8yZvlRYA3w2r6nkSQNjLpXMb2o6SCSpMFS9xDTOTv6emZ+sD9xJEmDYiJXMT0PuLJafyXwDeDeJkJJkro3kTcMOjozfwYQEYuByzLzzU0FkyR1q+5UG3OBR8asPwIM9z2NJGlg1H0FcTHwrYj4T3p3VJ8MXNRYKklS5+pexfT+iPgK8MfVpjdl5i3NxZIkda3uISaAfYCHMvMCYF1EHNpQJknSAKj7lqPnAecC76o2PQn4j6ZCSZK6V/cVxMnACcAvADJzA061IUlTWt2CeCQzk2rK74h4cnORJEmDoG5BXBoRHwNmRsRbgGvwzYMkaUqrexXTB6r3on4IOBz4u8y8utFkkqROjVsQEbEn8LXMfDFgKfTBsUuuY/2mLa2PO3vm9NbHlLTrGrcgMvOxiHg4ImZk5uY2Qk116zdtYc2Sl3cdQ5J2qO6d1L8EVkfE1VRXMgFk5tsaSSX1yeyZ0+GXMDyyovVxV474povatdUtiBXVh7RLWTmyABbT+iu2tgtJasIOCyIi5mbm2sxc3lYgSdJgGO8y1y+OLkTE5/s5cETsGRG3RMSXq/VDI+KmiLgrIi6JiL37OZ4kaWLGK4gYs3xYn8d+O3DHmPXzgaWZOQ94EDijz+NJkiZgvHMQuZ3lnRIRc4CXA+8HzomIABYAr6t2WQ4sBj7arzG31dWlpuDlppJ2DeMVxHMi4iF6rySmV8tU65mZ+01y3A8Bf8vW+ZwOBDZl5qPV+jpg9iR/di1eaipJO7bDgsjMPfs9YES8AnggM2+OiBeObi4Nv53vXwQsApg7d26/40mSKhN5P4h+ORY4ISLWAJ+jd2jpQ/TmeRotrDnAhtI3Z+ayzJyfmfOHhobayCtJu6XWCyIz35WZczJzGDgVuC4zXw9cD5xS7bYQuKLtbJKkrbp4BbE959I7YX03vXMSF3acR5J2a3XvpG5EZt4A3FAt3wM8v8s8kqStBukVhCRpgFgQkqQiC0KSVGRBSJKKLAhJUpEFIUkqsiAkSUUWhCSpyIKQJBV1eie11IoZc2HxjFaHXDMNWDoXzl7d6rhSP1kQmvo6eJIeHlnBmt+8/5W0a/IQkySpyIKQJBV5iElSXxy75DpW0ju81qbZM6ezstURdx8WhKS+WL9pC0yj9fd6Hx5ZAdNaHXK3YUFITeng6qnHje0VVNpJFoTUlC6foLsqJk0pnqSWJBVZEJKkIgtCklRkQUiSiiwISVKRBSFJKvIyV2kq6moG2xlzWx1TzbIgpKmoqxlsF7d7F7Wa5SEmSVKRBSFJKvIQk9SA2TOntz6r6ei4K0cWtD6upiYLQmpAV0/SXZSSpi4PMUmSiiwISVKRBSFJKmq9ICLikIi4PiLuiIjbIuLt1fYDIuLqiLir+rx/29kkSVt18QriUeCdmflM4BjgzIg4AhgBrs3MecC11bokqSOtF0Rm3peZ36mWfwbcAcwGTgSWV7stB05qO5skaatOz0FExDBwFHAT8JTMvA96JQIctJ3vWRQRqyJi1caNG9uKKkm7nc4KIiJ+G/g88I7MfKju92Xmssycn5nzh4aGmgsoSbu5TgoiIp5Erxw+nZlfqDbfHxEHV18/GHigi2ySpJ4urmIK4ELgjsz84JgvXQksrJYXAle0nU2StFUXU20cC/w5sDoibq22vRtYAlwaEWcAa4HXdJBNklRpvSAy85tAbOfLx7WZRZK0fd5JLUkqsiAkSUUWhCSpyIKQJBVZEJKkIgtCklRkQUiSiiwISVJRF3dSC2Dps2Dz2m7GnjG3m3GlpsyYC4tndDPu2avbH7clFkRXNq+FxZu7TiFNDV09SXdRSi3yEJMkqciCkCQVeYipq3MBngeQNOAsCM8FSFKRBSFNIbNnTmd4ZEVnY2tqsSCkKWTlyIKuI2gK8SS1JKnIgpAkFVkQkqQiz0FI0mR1NcXH6NgN30FuQUjSZHU5D1MLxeQhJklSkQUhSSqyICRJRRaEJKnIgpAkFVkQkqQiC0KSVGRBSJKKvFFO0i6t6ynOp/IMuhaEpF1al0/QXRVTWwaqICLipcAFwJ7AJzJzSceRJGm7unr1MnvmdFa2MM7AFERE7Al8BHgJsA74dkRcmZm3d5tMksq6evUyPLICpjU/ziCdpH4+cHdm3pOZjwCfA07sOJMk7bYGqSBmA/eOWV9XbZMkdWBgDjEBUdiWT9gpYhGwqFr9eUTcOekBz68W/r409KTNAn7Szx/YADP2x6BnHPR8YMZJCxj73DXRjE+ts9MgFcQ64JAx63OADdvulJnLgGVthZqoiFiVmfO7zrEjZuyPQc846PnAjP3SVMZBOsT0bWBeRBwaEXsDpwJXdpxJknZbA/MKIjMfjYi/Br5G7zLXT2bmbR3HkqTd1sAUBEBmXgVc1XWOnTSwh7/GMGN/DHrGQc8HZuyXRjJG5hPOA0uSNFDnICRJA8SCmKSIeGlE3BkRd0fESOHr50TE7RHxvYi4NiJqXVbWcsa/jIjVEXFrRHwzIo4YtIxj9jslIjIiWr2apMZjeHpEbKwew1sj4s1t5quTsdrntdXf420R8ZlByxgRS8c8hj+MiE0DmHFuRFwfEbdU/66PH8CMT62eb74XETdExJydGjAz/ZjgB72T6D8CDgP2Br4LHLHNPi8C9qmW3wpcMoAZ9xuzfALw1UHLWO23L/AN4EZg/iDlA04HPjzgf4vzgFuA/av1gwYt4zb7n0XvIpWBykjvOP9bq+UjgDUDmPEyYGG1vAC4eGfG9BXE5Iw7LUhmXp+ZD1erN9K7r2PQMj40ZvXJFG5MbFjd6VX+Afgn4JdthmPXmP6lTsa3AB/JzAcBMvOBAcw41mnAZ1tJtlWdjAnsVy3PoHCfVsPqZDwCuLZavr7w9QmxICZnotOCnAF8pdFET1QrY0ScGRE/ovcE/LaWso0aN2NEHAUckplfbjNYpe7v+dXVS/rLI+KQwtebVCfj04GnR8TKiLixmjW5TbX/vVSHYg8Frmsh11h1Mi4G3hAR6+hdbXlWO9F+o07G7wKvrpZPBvaNiAMnO6AFMTm1pgUBiIg3APOBf240UWHowrYnZMzMj2Tm04Bzgfc2nurxdpgxIvYAlgLvbC3R49V5DL8EDGfms4FrgOWNp3q8Ohn3oneY6YX0/nf+iYiY2XCusWr/e6F3g+zlmflYg3lK6mQ8DfhUZs4Bjgcurv5G21In498AL4iIW4AXAOuBRyc7oAUxObWmBYmIFwPvAU7IzF+1lG1UrYxjfA44qdFETzRexn2BI4EbImINcAxwZYsnqsd9DDPzp2N+tx8HnttStlF1fs/rgCsy89eZ+WPgTnqF0ZaJ/C2eSvuHl6BexjOASwEy87/oTbg9q5V0PXX+Hjdk5qsy8yh6zz1k5uZJj9jmSZap8kHvf2T30HspPHqy6Pe22ecoeieU5g1wxnljll8JrBq0jNvsfwPtnqSu8xgePGb5ZODGQXsMgZcCy6vlWfQOUxw4SBmr/Q4H1lDdnzWAj+NXgNOr5WfSe3JuLWvNjLOAParl9wPv26kx2/5FTJUPei8xf1iVwHuqbe+j92oBeocb7gdurT6uHMCMFwC3Vfmu39GTc1cZt9m31YKo+Rj+Y/UYfrd6DJ8xaI8hvUMTHwRuB1YDpw5axmp9MbCk7WwTeByPAFZWv+tbgT8dwIynAHdV+3wC+K2dGc87qSVJRZ6DkCQVWRCSpCILQpJUZEFIkoosCElSkQUhSSqyICRJRRaEJKno/wGy0SOrokilZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eee74d6e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design matrix\n",
    "X = pd.concat([df.constant, \n",
    "               df.c08_zmath, \n",
    "               df.c08_zlang, \n",
    "               pair_dums.iloc[:, 0:-1]],\n",
    "              axis=1)\n",
    "\n",
    "# logistic regression\n",
    "logit = sm.Logit(df['sat'], X)\n",
    "logit_results = logit.fit()\n",
    "logit_results.summary()\n",
    "\n",
    "# Fitted propensity score values\n",
    "df['propensity'] = logit_results.fittedvalues.apply(\n",
    "    lambda v: math.exp(v) / (1 + math.exp(v))\n",
    ")\n",
    "\n",
    "# histogram\n",
    "plt.figure()\n",
    "df[df.sat == 0].propensity.plot.hist(histtype='step')\n",
    "df[df.sat == 1].propensity.plot.hist(histtype='step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - X-axis is propensity score, and blue and orange lines represent propensity scores of children in CEB and SAT villages, respectively.\n",
    " - As shown in the histograms above, in every bin of propensity score, we have both SAT and CEB population. Therefore, the overlap condition is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.267</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:57</td>     <th>  Log-Likelihood:    </th> <td> -1148.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2300.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   711</td>      <th>  BIC:               </th> <td>   2309.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>    0.2835</td> <td>    0.092</td> <td>    3.068</td> <td> 0.003</td> <td>    0.100</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.1602</td> <td>    0.143</td> <td>    1.119</td> <td> 0.267</td> <td>   -0.125</td> <td>    0.445</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.222</td> <th>  Durbin-Watson:     </th> <td>   1.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.010</td> <th>  Jarque-Bera (JB):  </th> <td>   9.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.261</td> <th>  Prob(JB):          </th> <td>  0.0101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.193</td> <th>  Cond. No.          </th> <td>    2.71</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.267</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:33:57</td>     <th>  Log-Likelihood:    </th> <td> -1148.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2300.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   711</td>      <th>  BIC:               </th> <td>   2309.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>    0.2835</td> <td>    0.092</td> <td>    3.068</td> <td> 0.003</td> <td>    0.100</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.1602</td> <td>    0.143</td> <td>    1.119</td> <td> 0.267</td> <td>   -0.125</td> <td>    0.445</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.222</td> <th>  Durbin-Watson:     </th> <td>   1.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.010</td> <th>  Jarque-Bera (JB):  </th> <td>   9.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.261</td> <th>  Prob(JB):          </th> <td>  0.0101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.193</td> <th>  Cond. No.          </th> <td>    2.71</td>\n",
       "</tr>\n",
       "</table>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IPW weights\n",
    "df['ipw_weight'] = df.apply(\n",
    "    lambda row: row.sat / row.propensity \n",
    "                + (1 - row.sat) / (1 - row.propensity),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Multiply weights by test_wgt\n",
    "df.ipw_weight = test_wgt * df['ipw_weight']\n",
    "\n",
    "# WLS\n",
    "wls_ipw = sm.WLS(df['c10_zmath'],\n",
    "                 df[['constant', 'sat']],\n",
    "                 weights=df.ipw_weight\n",
    "                 ).fit(cov_type='cluster',\n",
    "                       cov_kwds={'groups': df['feeder_school']},\n",
    "                       use_t=True)\n",
    "\n",
    "wls_ipw.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - By using the Inverse Probability Weighting, we can account for how different covariates are distributed between the treatment and control groups. It corrects our estimate by up-weighting under-represented treated units in treatment and control groups, potentially reducing the bias of our estimate of the average treatment effect.\n",
    " - The coefficient on *sat* is 0.1602, and it is not significant even at the 10 percent level. We cannot conclude that residing in a SAT village resulted in higher 2010 math test scores than residing in a CEB village. In the paper, this is the second specifictation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - By studying the children for longer period of time, one can obtain more interesting data, such as high school/college enrollment and adulthood wage. These data would make it possible to estimate not only the short-term effects, but also the long-term effects of SAT and CEB schools.\n",
    " - More data on school-level peer composition, such as average parental income and schooling years, would be helpful. The paper uses mean of baseline composite test score as a proxy for peer composition. SInce peer pressure is one of the important factors in determining student achievement, these data can potentially provide insights into why SATs and CEBs show different effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
